# =============================================================================
# DATABASE PROVIDER CONFIGURATION
# =============================================================================
# DB_PROVIDER controls where lamdis-runs stores test run data.
# Options:
#   - "local"    : In-memory/JSON files (no external DB required, ephemeral)
#   - "mongo"    : MongoDB (requires MONGO_URL)
#   - "postgres" : PostgreSQL via Prisma (requires DATABASE_URL)
#
# If DB_PROVIDER is not set, auto-detection is used:
#   - DATABASE_URL starting with "postgres" → postgres
#   - MONGO_URL set → mongo
#   - Otherwise → local (JSON-only mode)
DB_PROVIDER=local

# MongoDB connection (required when DB_PROVIDER=mongo)
MONGO_URL=mongodb://localhost:27017/lamdis

# PostgreSQL connection (required when DB_PROVIDER=postgres)
# Run `npx prisma generate && npx prisma db push` after setting this
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/lamdis

# =============================================================================
# SERVER CONFIGURATION
# =============================================================================
PORT=3101
LAMDIS_API_TOKEN=change-me-shared-token
LAMDIS_HMAC_SECRET=change-me-hmac-secret
API_BASE_URL=http://localhost:3001
ENC_SECRET=

# =============================================================================
# LLM PROVIDER CONFIGURATION
# =============================================================================
# Which provider lamdis-runs should use for conversations and judging.
# Supported: "openai", "bedrock".
LAMDIS_LLM_PROVIDER=openai

# OpenAI configuration (used when LAMDIS_LLM_PROVIDER=openai)
# Set OPENAI_API_KEY in your real env, not in source control.
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4.1-mini

# AWS Bedrock configuration (used when LAMDIS_LLM_PROVIDER=bedrock)
AWS_REGION=us-east-1
BEDROCK_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0

# Optional: separate judge provider/model; if empty, reuse LAMDIS_LLM_PROVIDER.
LAMDIS_JUDGE_PROVIDER=
LAMDIS_JUDGE_MODEL=
