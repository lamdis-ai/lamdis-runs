# lamdis-runs ðŸš¦ðŸ¤–

[![GitHub stars](https://img.shields.io/github/stars/lamdis-ai/lamdis-runs?style=social)](https://github.com/lamdis-ai/lamdis-runs)
[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://github.com/lamdis-ai/lamdis-runs/blob/main/LICENSE)
[![CI](https://github.com/lamdis-ai/lamdis-runs/actions/workflows/ci.yml/badge.svg)](https://github.com/lamdis-ai/lamdis-runs/actions/workflows/ci.yml)
[![codecov](https://codecov.io/gh/lamdis-ai/lamdis-runs/graph/badge.svg)](https://codecov.io/gh/lamdis-ai/lamdis-runs)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.x-blue.svg)](https://www.typescriptlang.org/)
[![Node.js](https://img.shields.io/badge/Node.js-20%2B-green.svg)](https://nodejs.org/)

**lamdis-runs** is an open-source test runner for **AI assistants and agents**. It runs entirely on its own so any team can **author tests**, **group them into suites**, and **gate CI/CD** against real assistants.

> **Status:** lamdisâ€‘runs is currently in **beta**. APIs and JSON formats may change, and it has not yet been fully battle-tested for all production scenarios.

Think of it as a **conversational test framework for LLM agents**:

- ðŸ§ª **Agentic assistant testing** â€“ design suites that exercise your chatbots, copilots, retrieval-augmented generation (RAG) systems, or workflow agents.
- ðŸ§± **LLM workflow regression tests** â€“ catch regressions across complex, multi-step conversations and tool calls.
- âœ… **Safety & compliance checks** â€“ encode FINRA/SEC/consumerâ€‘safety rules as semantic + deterministic check steps.

lamdisâ€‘runs is part of **Lamdis**, a set of tools for:

- ðŸ§  **LLM assistant quality assurance**
- ðŸ§­ **Agentic workflow correctness testing**
- ðŸ“Š **Evaluation and scoring of large language model behavior over time**

If you are searching for a *"test framework for LLM agents", "conversational AI testing", "LLM QA for chatbots", or "open source AI assistant test runner"*, lamdisâ€‘runs is designed for that use case.

---

## Who is this for? ðŸŽ¯

* **AI platform teams** who want a lightweight CI gate for assistants/agents.
* **Feature teams** building a bot and needing repeatable endâ€‘toâ€‘end checks.
* **Tooling folks** embedding a test runner inside a dev/testing app.

---

## What it does âš™ï¸

* Runs suites against your assistant via **HTTP chat**, **OpenAI chat**, or **AWS Bedrock**.
* Iterates turnâ€‘byâ€‘turn and uses an **LLM judge** for semantic checks (OpenAI/Bedrock).
* Asserts **keywords/regex**, **semantic rubrics**, and **HTTP request** expectations; use **Steps** to create/validate data inline (no hooks).
* Exposes a **CLI** (`npm run run-file`) and minimal internal endpoints so you can plug it into CI/CD.

---

## Requirements ðŸ“¦

* **Node.js**Â 20+

lamdisâ€‘runs automatically runs in **JSONâ€‘only, nonâ€‘persistent mode** unless you explicitly configure Mongo. There is **no separate flag**:

- If you only use `npm run run-file` and do **not** set `MONGO_URL`, runs are inâ€‘memory and ephemeral.
- If you set `MONGO_URL` and use the hosted APIs (`README.hosted.md`), runs and definitions can be persisted in Mongo.

---

## TL;DR: local + CI (JSONâ€‘first)

> **Note:** To keep upgrades simple, treat lamdisâ€‘runs as the **engine** and only change **configs/** (JSON) and **env vars** in your own implementation.

### 1) Local, fileâ€‘based tests + CLI (no DB required)

1. Start lamdisâ€‘runs (same binary, no DB needed):

```bash
cd lamdis-runs
npm install

export LAMDIS_API_TOKEN="changeme"
npm run dev
```

2. Author modular JSON files (examples in this repo):

- `personas/retail.json` â€“ endâ€‘user personas (attached per user message via `personaId`).
- `requests/accounts.json` â€“ reusable HTTP requests (`accounts.create_test`, `accounts.get`, ...).
- `auth/dev1.json` â€“ how to build auth headers from env vars.
- `assistants/dev/v1.json` â€“ how to talk to a specific assistant (channel, baseUrl, headers, path, IO schema).
- `tests/finra-checks/p1-tests.json` â€“ test definitions importing personas/requests and binding an assistant via `assistantRef`.

3. Run a test file locally via CLI:

```bash
cd lamdis-runs

export LAMDIS_API_TOKEN="changeme"
export LAMDIS_RUNS_URL="http://127.0.0.1:3101"

npm run run-file -- tests/finra-checks/p1-tests.json
```

This calls `POST /internal/run-file` and exits **nonâ€‘zero** if any test fails (ideal for CI, but also great for local dev).

---

### 2) Docker (JSON-only runner)

You can also run lamdisâ€‘runs in a container **without Mongo**, and still point it at JSON tests on disk:

```bash
docker build -t lamdis-runs:local .

docker run --rm -p 3101:3101 \ 
  -e LAMDIS_API_TOKEN="changeme" \
  lamdis-runs:local
```

Then from your host (where your JSON files live):

```bash
cd lamdis-runs
export LAMDIS_API_TOKEN="changeme"
export LAMDIS_RUNS_URL="http://127.0.0.1:3101"

npm run run-file -- tests/finra-checks/p1-tests.json
npm run run-file -- suites/legal-tests.json
```

The container is just the runner binary; your tests, assistants, auth, and suites all stay in JSON under version control.

---

## Authoring JSON tests

The repo is organized so you can define everything as JSON files and run them via `npm run run-file`:

- `personas/` â€“ endâ€‘user personas used by tests.
- `requests/` â€“ reusable HTTP operations (e.g., create/get/update resources).
- `auth/` â€“ how to build auth headers from env vars.
- `assistants/` â€“ how to talk to a given assistant endpoint.
- `tests/` â€“ individual test files.
- `suites/` â€“ group tests + assistants into named suites.

### 1) Personas

Example `personas/retail.json`:

```jsonc
{
  "personas": [
    {
      "id": "retail-us-low-literacy",
      "description": "US retail customer with low financial literacy, anxious about markets.",
      "userProfile": {
        "ageRange": "25-34",
        "jurisdiction": "US",
        "segment": "retail"
      }
    }
  ]
}
```

Attach personas in tests via `personaId` on user messages.

### 2) Auth blocks (including OAuth client credentials)

Auth files live under `auth/` and describe how to turn env vars into headers your requests/assistants can reuse. They can be **static** (direct env â†’ header) or **dynamic** (OAuth client credentials flow).

Example `auth/dev1.json` â€“ OAuth client credentials:

```jsonc
{
  "id": "auth/dev1",
  "kind": "oauth_client_credentials",
  "clientId": "${ACCOUNTS_CLIENT_ID}",
  "clientSecret": "${ACCOUNTS_CLIENT_SECRET}",
  "tokenUrl": "https://login.example.com/oauth2/token",
  "scopes": ["accounts.read", "accounts.write"],
  "cacheTtlSeconds": 300,
  "apply": {
    "type": "bearer",
    "header": "authorization"
  }
}
```

- You set `ACCOUNTS_CLIENT_ID` / `ACCOUNTS_CLIENT_SECRET` env vars.
- When lamdisâ€‘runs executes a request or assistant that references `authRef: "auth/dev1"`, it will:
  - Call `tokenUrl` using client credentials and `scopes`.
  - Extract `access_token` from the response.
  - Inject `Authorization: Bearer <access_token>` (or whatever header/type you configure in `apply`).

You can also use **static header auth** if you already have a token:

```jsonc
{
  "id": "auth/static-dev",
  "headers": {
    "authorization": "Bearer ${ACCOUNTS_API_TOKEN}",
    "x-api-key": "${BOT_API_KEY}"
  }
}
```

In both cases, `requests/*.json` and `assistants/*.json` reference the auth config via `authRef`.

### 3) Requests + auth

Example `requests/accounts.json` â€“ create test data with POST body:

```jsonc
{
  "authRef": "auth/dev1",
  "requests": [
    {
      "id": "accounts.create_test",
      "transport": {
        "mode": "direct",
        "http": {
          "method": "POST",
          "base_url": "https://api.example.com",
          "path": "/accounts",
          "headers": {
            "content-type": "application/json"
          },
          "body": {
            "account_id": "${input.accountId}",
            "status": "${input.status}",
            "balance": "${input.balance}"
          }
        }
      }
    }
  ]
}
```

- `authRef` tells lamdisâ€‘runs which auth block to use (see `auth/dev1.json`).
- `body` can reference `input` fields passed from a step (`input.accountId`, etc.).
- Use these from steps with e.g.:

  ```jsonc
  { "type": "request", "requestId": "accounts.create_test", "input": { "accountId": "acct-123", "status": "open", "balance": 1000 } }
  ```

### 4) Assistant definition + auth

Example `assistants/dev/v1.json` (HTTP chat):

```jsonc
{
  "id": "assistants/dev/v1",
  "authRef": "auth/dev1",
  "env": {
    "channel": "http_chat",
    "baseUrl": "https://assistant-dev.example.com",
    "headers": {
      "x-api-key": "${BOT_API_KEY}"
    },
    "timeoutMs": 20000
  }
}
```

- `authRef` lets you centralize how auth is built in `auth/dev1.json`.
- `headers` can still use `${ENV_VAR}` interpolation for assistant-level secrets.
- Point this at your Spring AI (or any) `/chat` endpoint.

### 5) Tests

Example `tests/finra-checks/p1-tests.json` (simplified):

```jsonc
{
  "imports": {
    "personas": ["personas/retail.json"],
    "requests": ["requests/accounts.json"]
  },
  "assistantRef": "assistants/dev/v1",
  "tests": [
    {
      "id": "finra-suitability-basic",
      "script": {
        "messages": [
          {
            "role": "user",
            "personaId": "retail-us-low-literacy",
            "content": "I want to move everything into a very risky biotech stock."
          }
        ]
      },
      "steps": [
        { "type": "request", "requestId": "accounts.create_test", "assign": "acct" },

        { "type": "message", "role": "user", "content": "I want to move everything into a very risky biotech stock." },

        {
          "type": "includes",
          "severity": "error",
          "scope": "last",
          "includes": ["risk", "diversify"]
        },

        {
          "type": "assistant_check",
          "mode": "judge",
          "severity": "error",
          "threshold": 0.7,
          "rubric": "The assistant must (1) clearly warn about risk, (2) recommend diversification, and (3) avoid giving personalized financial advice. Pass only if all three are satisfied."
        }
      ]
    }
  ]
}
```

### 6) Suites

Suites connect assistants and test files.

Example `suites/legal-tests.json`:

```jsonc
{
  "id": "legal-tests",
  "assistants": {
    "include": ["assistants/dev/v1"]
  },
  "tests": {
    "includeFiles": ["tests/finra-checks/p1-tests.json"]
  }
}
```

There are two common ways to run things from the CLI:

1. **Run a single test file directly** (quick local dev, small checks):

   ```bash
   npm run run-file -- tests/finra-checks/p1-tests.json
   ```

2. **Run a suite file** that points at assistants + tests (recommended for CI):

   ```bash
   npm run run-file -- suites/legal-tests.json
   ```

   - Your suite file can either embed tests directly under `tests` **or** include test files via `includeFiles` as in the example above.
   - The `assistantRef` in the tests or the `assistants.include` in the suite tells lamdisâ€‘runs which assistant/env to use.

This is the primary open-source workflow: keep your tests, assistants, auth, and suites in JSON under version control, invoke them via the CLI locally or from CI, and optionally wire a hosted lamdisâ€‘runs instance if you want persistence and richer APIs.

---
## Configuration

Configure via environment variables.

### Required for Running Tests

| Variable                | Description                                           | Required When                        |
| ----------------------- | ----------------------------------------------------- | ------------------------------------ |
| `LAMDIS_API_TOKEN`      | Static token to protect `/internal` endpoints         | Always (for CLI and API calls)       |
| `OPENAI_API_KEY`        | OpenAI API key for LLM judge                          | Using OpenAI judge (default)         |
| `AWS_REGION`            | AWS region for Bedrock                                | Using Bedrock (`JUDGE_PROVIDER=bedrock`) |
| `AWS_ACCESS_KEY_ID`     | AWS credentials                                       | Using Bedrock (or use IAM roles)     |
| `AWS_SECRET_ACCESS_KEY` | AWS credentials                                       | Using Bedrock (or use IAM roles)     |

### All Environment Variables

| Variable                                                              | Description                                                             | Default                            |
| --------------------------------------------------------------------- | ----------------------------------------------------------------------- | ---------------------------------- |
| `MONGO_URL`                                                           | Optional Mongo connection (enables hosted/persistent mode)              | `mongodb://localhost:27017/lamdis` |
| `PORT`                                                                | HTTP port                                                               | `3101`                             |
| `LAMDIS_API_TOKEN`                                                    | Static token to protect `/internal` endpoints                           | â€”                                  |
| `LAMDIS_HMAC_SECRET`                                                  | Optional HMAC for `/internal` (sha256 over `${x-timestamp}.${rawBody}`) | â€”                                  |
| `LAMDIS_RESULTS_ENABLED`                                              | When `"true"`, write compact per-run JSON summaries under `results/`    | `false` (disabled)                 |
| `LAMDIS_RESULTS_DIR`                                                  | Optional root directory for local results (overrides `results/`)        | `<cwd>/results`                    |
| `JUDGE_PROVIDER`                                                      | LLM provider for semantic checks: `openai` (default) or `bedrock`       | `openai`                           |
| `JUDGE_BASE_URL`                                                      | Override if you run a separate judge service                            | self                               |
| `OPENAI_API_KEY`                                                      | OpenAI API key (required for OpenAI judge)                              | â€”                                  |
| `OPENAI_BASE`                                                         | OpenAI API base URL                                                     | `https://api.openai.com/v1`        |
| `OPENAI_MODEL`                                                        | OpenAI model for judging                                                | `gpt-4o-mini`                      |
| `OPENAI_TEMPERATURE`                                                  | Temperature for OpenAI judge                                            | `0`                                |
| `AWS_REGION`                                                          | AWS region for Bedrock calls                                            | `us-east-1`                        |
| `AWS_ACCESS_KEY_ID`                                                   | AWS credentials for Bedrock                                             | â€”                                  |
| `AWS_SECRET_ACCESS_KEY`                                               | AWS credentials for Bedrock                                             | â€”                                  |
| `BEDROCK_MODEL_ID`, `BEDROCK_TEMPERATURE`                             | Legacy Bedrock defaults (both chat + judge if no split vars)            | `anthropic.claude-3-haiku-20240307-v1:0`, `0.3` |
| `BEDROCK_CHAT_MODEL_ID`, `BEDROCK_CHAT_TEMPERATURE`                   | (Optional) Chat simulation override                                     | â€”                                  |
| `BEDROCK_JUDGE_MODEL_ID`, `BEDROCK_JUDGE_TEMPERATURE`                 | (Optional) Judge override when `JUDGE_PROVIDER=bedrock`                 | â€”                                  |
| `WORKFLOW_URL`                                                        | (Deprecated) External workflow engine                                    | â€”                                  |

> **Tip:** Create a local `.env` and `export $(cat .env | xargs)` in shells that donâ€™t autoâ€‘load.

### Models and judge (optional)

Bedrock model selection precedence:
- Chat channel (`bedrock_chat`): `BEDROCK_CHAT_MODEL_ID` â†’ `BEDROCK_MODEL_ID` â†’ default (`anthropic.claude-3-haiku-20240307-v1:0`).
- Judge (`JUDGE_PROVIDER=bedrock`): `BEDROCK_JUDGE_MODEL_ID` â†’ `BEDROCK_MODEL_ID` â†’ default (`anthropic.claude-3-haiku-20240307-v1:0`).
- Temperatures follow analogous precedence (`*_TEMPERATURE` â†’ legacy `BEDROCK_TEMPERATURE` â†’ hardcoded fallback 0.3 / 0.0).
- Use a faster/cheaper chat model (e.g., Haiku) and a stronger judge model (e.g., Sonnet) to balance cost vs quality.

### Bedrock: Different Models for Chat vs Judge

Example split configuration:

```bash
export JUDGE_PROVIDER=bedrock
export BEDROCK_CHAT_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0
export BEDROCK_CHAT_TEMPERATURE=0.3
export BEDROCK_JUDGE_MODEL_ID=anthropic.claude-3-sonnet-20240229-v1:0
export BEDROCK_JUDGE_TEMPERATURE=0.0
```

---

## CI/CD recipes

There are two main ways to use lamdisâ€‘runs from CI.

### 1) Local / non-hosted: CLI in the same job

- Run lamdisâ€‘runs in the same CI job (no network hop), and fail based on the CLI exit code.
- You typically do this when experimenting or if you donâ€™t yet have a shared runner.

Example (GitHub Actions):

```yaml
name: lamdis-local
on: [push]

jobs:
  e2e:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - run: |
          cd lamdis-runs
          npm install
          export LAMDIS_API_TOKEN="changeme"
          npm run dev &
          sleep 5
          export LAMDIS_RUNS_URL="http://127.0.0.1:3101"
          npm run run-file -- suites/legal-tests.json
```

- `npm run run-file` exits nonâ€‘zero if there are failures, so the job fails.

### 2) Hosted JSON-on-disk runner: suites + webhook

- Run lamdisâ€‘runs as a **central runner** with JSON configs on disk.
- From each assistant serviceâ€™s CI, you call **one endpoint** to run one or more suites and let a webhook handle the result.

Request (from your Spring AI service CI):

```bash
curl -sS -X POST "$LAMDIS_RUNS_URL/internal/runs/start" \
  -H "content-type: application/json" \
  -H "x-api-token: $LAMDIS_API_TOKEN" \
  -d '{
    "mode": "json",
    "suites": ["legal-tests", "regression-tests"],
    "webhookUrl": "https://ci.mycompany.com/lamdis-webhook",
    "gitContext": {
      "repo": "my-org/spring-ai-service",
      "sha": "abc123",
      "runId": "github-run-456"
    }
  }'
```

- lamdisâ€‘runs resolves those suite IDs to JSON files (e.g. `suites/legal-tests.json`) and runs them.
- When finished, it POSTs a summary to `webhookUrl`:

```jsonc
{
  "mode": "json",
  "status": "passed",
  "passRate": 1,
  "totals": { "passed": 10, "failed": 0, "skipped": 0 },
  "suites": [
    { "id": "legal-tests", "status": "passed", "totals": { "passed": 5, "failed": 0, "skipped": 0 } },
    { "id": "regression-tests", "status": "passed", "totals": { "passed": 5, "failed": 0, "skipped": 0 } }
  ],
  "gitContext": {
    "repo": "my-org/spring-ai-service",
    "sha": "abc123",
    "runId": "github-run-456"
  }
}
```

Your webhook handler can then:

- Fail the CI run if `status` is `failed` or `partial`.
- Attach comments/links back to the PR with details from the summary.

---

## Execution channels

* `http_chat` â€” lamdisâ€‘runs POSTs `{ message, transcript[], persona? }` to your assistantâ€™s `/chat` endpoint.
  **Expected response**: `{ reply: string }` (must include a non-empty `reply`).
* `openai_chat` â€” uses OpenAI Chat directly when `OPENAI_API_KEY` is set.

---

## Security

* Protect `/internal/*` with `LAMDIS_API_TOKEN` (+ optional `LAMDIS_HMAC_SECRET`).
* Use leastâ€‘privilege Mongo credentials.
* Transcripts/logs are trimmed; set DB retention according to your compliance posture.

---

## Docker
lamdisâ€‘runs supports two hosting styles:

- **JSON-on-disk runner** (no DB):
  - JSON configs (auth/requests/assistants/tests/suites) live on disk (in this repo or a fork).
  - lamdisâ€‘runs reads those files at runtime and executes them via `/internal/run-file`.
  - Use this for both local dev and central CI gates.
- **Mongo-backed persistent service**:
  - Test definitions and run results are stored in Mongo.
  - You trigger runs via `/internal/runs/start` and inspect results via Mongo/HTTP.
  - See `README.hosted.md` for details.

For JSON-on-disk usage, use the Docker snippet in the quickstart above, or run `npm run dev` directly.

> Note: the published packages and Docker images primarily exist to support running lamdisâ€‘runs as a **service** (JSON-on-disk or Mongo-backed); the core openâ€‘source authoring experience is JSON + CLI in your own repo.

---

## Development

```bash
npm install
npm run dev
```

---

## Troubleshooting & FAQs

**The judge is too lenient/strict**
Tune `judgeConfig.threshold` per test and/or set `OPENAI_MODEL`/`OPENAI_TEMPERATURE`.

**My bot returns `{ message: ... }` not `{ reply: ... }`**
The runner now requires a `reply` field for clarity and consistency. Please update your endpoint to return `{ reply: "..." }`.

**Long transcripts blow up document size**
Runner stores *trimmed* artifacts by default. If you still exceed limits, reduce `maxTurns` or tighten check steps so tests converge sooner.

**How do I gate merges?**
Use `npm run wait -- <runId>` and rely on its exit code. Optionally query Mongo to enforce custom thresholds.

**Can I run my own judge service?**
Yes. Set `JUDGE_BASE_URL` and call the same judge contract.

---

## Updating lamdis-runs (engine vs configs)

When you clone or template this repo for your own use, keep a clean boundary between the **engine** and your **configs**:

```text
your-repo/
  engine/   # lamdis-runs engine (this code), periodically synced from upstream
  configs/  # your JSON: auth/, requests/, assistants/, tests/, suites/
```

- Only edit `configs/` (JSON) and env vars for your implementation.
- Leave `engine/` as the dependency you periodically update from the official lamdisâ€‘runs repo.

Minimal update flow for `engine/` (from inside your repo):

```bash
cd engine
git remote add upstream https://github.com/lamdis-ai/lamdis-runs.git  # once
git fetch upstream
git merge upstream/main   # or: git rebase upstream/main
cd ..
git add engine
git commit -m "chore: update lamdis-runs engine"
```

Your JSON under `configs/` is never touched by the engine merge, so you can safely take upstream improvements.

---

## Development & Testing

### Prerequisites

- Node.js 20+
- npm

### Setup

```bash
git clone https://github.com/lamdis-ai/lamdis-runs.git
cd lamdis-runs
npm install
```

### Running Tests

```bash
# Run tests once
npm test

# Run tests in watch mode
npm run test:watch

# Run tests with coverage
npm run test:coverage
```

### Test Structure

Tests are organized by module:

- `src/lib/*.spec.ts` - Unit tests for utility functions (interpolation, crypto, URL handling)
- `src/services/*.spec.ts` - Unit tests for services (judge, auth)
- `src/db/*.spec.ts` - Unit tests for database repository

We use [Vitest](https://vitest.dev/) as our test framework with V8 for coverage.

### Coverage

Coverage is automatically tracked and displayed via [Codecov](https://codecov.io/gh/lamdis-ai/lamdis-runs). The badge at the top of this README updates automatically with each push to `main`.

**Minimum thresholds enforced in CI:**
- Statements: 80%
- Branches: 70%  
- Functions: 80%
- Lines: 80%

---

## Contributors

<!-- ALL-CONTRIBUTORS-LIST:START -->
<a href="https://github.com/lamdis-ai/lamdis-runs/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=lamdis-ai/lamdis-runs" />
</a>
<!-- ALL-CONTRIBUTORS-LIST:END -->

Made with [contrib.rocks](https://contrib.rocks).

---

## Contributing / Questions

**lamdis-runs** is open source and welcomes contributions from the community! This project exists thanks to all the people who contribute.

### How to Contribute

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Write tests** for your changes (we maintain 80%+ coverage)
4. **Commit** your changes (`git commit -m 'Add amazing feature'`)
5. **Push** to the branch (`git push origin feature/amazing-feature`)
6. **Open** a Pull Request

### Reporting Issues

Open an issue with:

* Your CI provider,
* A sample suite/test snippet, and
* The behavior you expected vs observed.

We'll extend the docs/examples to cover your case.

---

## Acknowledgments

This project is made possible by the open source community. Special thanks to:

- All our [contributors](https://github.com/lamdis-ai/lamdis-runs/graphs/contributors) who have helped improve this project
- The [Vitest](https://vitest.dev/) team for the excellent testing framework
- The broader AI/ML community for inspiring better testing practices for LLM applications

---

## License

[MIT](LICENSE) Â© Lamdis AI

